{
  "cells": [
    {
      "metadata": {
        "id": "V9Z1qR1Lsd9I"
      },
      "cell_type": "markdown",
      "source": [
        "# Part II: Gradient-based Explanations"
      ]
    },
    {
      "metadata": {
        "id": "yo3Gj_G_sd9M"
      },
      "cell_type": "markdown",
      "source": [
        "#### Run these cells to mount the notebook to the drive folder and set the correct paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQFt74meTJ4z"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXw08rQKUpkc"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "repo_path = \"/content/drive/MyDrive/AISEC-SummerSchool-2025/XAI for Security/drebin_xai\"\n",
        "data_path = \"/content/drive/MyDrive/AISEC-SummerSchool-2025/XAI for Security/drebin_xai/data\"\n",
        "sys.path.append(f\"{repo_path}/src\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzdJgr4Ac31x"
      },
      "source": [
        "#### Run these cells to install and load necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE6nDpAnszIe"
      },
      "outputs": [],
      "source": [
        "# Install model and training environment\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "metadata": {
        "id": "G2Q9zl_ssd9N"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Load necesarry packages\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics.classification import Accuracy\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "from sklearn.datasets import load_svmlight_file"
      ]
    },
    {
      "metadata": {
        "id": "c4RzY99Gsd9N"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 Gradient-based explanations\n",
        "\n",
        "In this section we will implement two approaches to use gradients to explain a machine learning method.\n",
        "\n",
        "### 2.1 Vanilla Gradient\n",
        "\n",
        "For vanilla gradients, the gradients of a model should be calculated w.r.t. to an input instance and the corresponding model output. This is not so different from the warm-up task, as a machine learning model can also be considered a large function.\n",
        "\n",
        "### 2.2 Gradient x Input\n",
        "\n",
        "In the previous approach, only the gradients of the model prediction in relation to the input were analyzed. This type of explanation can also be interpreted as sensitivity. They reflect how the prediction responds to infinitesimal changes in the inputs. However, the values of the input features have not been taken into account so far. As the name of the method suggests, the values of an input instance are multiplied by the gradients."
      ]
    },
    {
      "metadata": {
        "id": "jRPZNWORsd9N"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "class GradientExplainer:\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "    self.model.eval()\n",
        "\n",
        "  def __prep_input(input):\n",
        "      # Ensure that input is tensor\n",
        "      if isinstance(input, np.ndarray):\n",
        "        input = torch.from_numpy(input)\n",
        "        input = input.float()\n",
        "      input = input.unsqueeze(0).clone().detach()\n",
        "      return input\n",
        "\n",
        "  def __get_logit(model_output):\n",
        "    \"\"\"\n",
        "    Extracts the logit corresponding to the predicted class\n",
        "    from a model's output tensor.\n",
        "\n",
        "    Args:\n",
        "        model_output (torch.Tensor): Output from the model,\n",
        "                                     typically shape [1, num_classes]\n",
        "                                     or scalar.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The logit (un-normalized score) for the\n",
        "                      predicted class.\n",
        "    \"\"\"\n",
        "    # Remove the batch dimension if present (e.g., [1, num_classes] → [num_classes])\n",
        "    output = model_output.squeeze(0)\n",
        "\n",
        "    # If the output is a scalar (e.g., regression task), just return it\n",
        "    if output.dim() == 0:\n",
        "        return output\n",
        "\n",
        "    # Otherwise (classification task), get the logit for the predicted class\n",
        "    predicted_class = output.argmax().item()  # index of max logit\n",
        "    return output[predicted_class]\n",
        "\n",
        "  def vanilla_gradient(self, input, target=None):\n",
        "    \"\"\"\n",
        "    Compute a vanilla gradient saliency map for a single input.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : torch.Tensor\n",
        "        Model input. Expected shape is `[1, ...]` (a single example with a\n",
        "        leading batch dimension).\n",
        "    target : Optional[int], default=None\n",
        "        Intended class index to explain.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "        Gradient with respect to the input of the first (and only) item in the\n",
        "        batch. Shape matches `input[0]` (i.e., the input without its batch\n",
        "        dimension). Returned as a NumPy array.\n",
        "    \"\"\"\n",
        "    input = GradientExplainer.__prep_input(input)\n",
        "\n",
        "    #######################\n",
        "    # TODO: YOUR CODE HERE\n",
        "    #######################\n",
        "    # Compute forward pass\n",
        "    input.requires_grad = True\n",
        "    output = self.model(input)\n",
        "\n",
        "    logit = self.__get_logit(output)\n",
        "\n",
        "    #######################\n",
        "    # TODO: YOUR CODE HERE\n",
        "    #######################\n",
        "    # Compute gradient starting from logit\n",
        "    self.model.zero_grad()\n",
        "    logit.backward()\n",
        "\n",
        "    return input.grad[0].detach().cpu().numpy().squeeze()\n",
        "\n",
        "  def gradient_x_input(self, input, target=None):\n",
        "    \"\"\"\n",
        "    Compute Gradient ⊙ Input attribution for a single input.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : torch.Tensor\n",
        "        Model input. Expected shape `[1, ...]` (single example with batch dim).\n",
        "    target : Optional[int], default=None\n",
        "        Intended class index to explain.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "        Element-wise product of input and its gradient, with the batch\n",
        "        dimension removed.\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    grad = self.vanilla_gradient(input)\n",
        "    gradients = input * grad\n",
        "    return gradients.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn6afmBrc6Kc"
      },
      "outputs": [],
      "source": [
        "#@title Solution Vanilla Gradient\n",
        "def vanilla_gradient(self, input, target=None):\n",
        "    input = GradientExplainer.prep_input(input)\n",
        "\n",
        "    # Your code here\n",
        "    input.requires_grad = True\n",
        "    # Forward-pass\n",
        "    output = self.model(input)\n",
        "\n",
        "    # Remove batch dimension\n",
        "    output = output.squeeze(0)\n",
        "    # Determine scalar for backprop\n",
        "    if output.dim() == 0:\n",
        "        logit = output\n",
        "    else:\n",
        "        predicted_class = output.argmax().item()\n",
        "        logit = output[predicted_class]\n",
        "\n",
        "    # Your code here\n",
        "    # Compute gradient starting from logit\n",
        "    self.model.zero_grad()\n",
        "    logit.backward()\n",
        "\n",
        "    return input.grad[0].detach().cpu().numpy().squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVCro205dj2h"
      },
      "outputs": [],
      "source": [
        "#@title Solution Gradient X Input\n",
        "def gradient_x_input(self, input, target=None):\n",
        "    grad = self.vanilla_gradient(input)\n",
        "    gradients = input * grad\n",
        "    return gradients.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc6V7gg0fpZL"
      },
      "source": [
        "### 2.3 Difference between Vanilla Gradient and Gradient x Input explanation\n",
        "\n",
        "As mentioned earlier, vanilla gradients and the gradient × input method differ conceptually. Vanilla gradients measure the sensitivity of the model’s prediction with respect to each input feature, capturing how much a small change in a feature, e.g. a pixel, would affect the output. In contrast, the gradient × input approach also considers the magnitude of the input feature itself, providing a form of feature attribution: it indicates how much the actual value of a feature contributes to the model’s prediction.\n",
        "\n",
        "In the following notebook you will apply the implementations form above to two examples that illustrate the difference.\n",
        "\n",
        "**Example 1:**\n",
        "\n",
        "Lets assume we have a simple model:\n",
        "\n",
        "$f(x) = 2 * x_1 + 0.1 * x_2 $, and an input $ x = [1, 10] $\n",
        "\n",
        "Use your own implementations from above to compute the different gradient explanations and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZMUR92Fi8ck"
      },
      "outputs": [],
      "source": [
        "# Input\n",
        "x = torch.tensor([1., 10.])\n",
        "\n",
        "# Define toy model that represents the function f(x) = 2*x_1+0.1*x_2\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Linear layer without bias that takes two values and outputs one input\n",
        "        self.linear = nn.Linear(2, 1, bias=False)\n",
        "        # Manually set weights to match f(x1, x2) = 2*x1 + 0.1*x2\n",
        "        with torch.no_grad():\n",
        "            self.linear.weight.copy_(torch.tensor([[2.0, 0.1]]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Create model\n",
        "model = SimpleModel()\n",
        "\n",
        "\n",
        "# TODO: compute vanilla gradient and gradient x input for the SimpleModel with regards to x\n",
        "# Your solution here\n",
        "explainer = GradientExplainer(model)\n",
        "vanilla_gradient = explainer.vanilla_gradient(x)\n",
        "gradient_x_input = explainer.gradient_x_input(x)\n",
        "\n",
        "print(f\"Vanilla Gradient: {vanilla_gradient}\")\n",
        "print(f\"Gradient x Input: {gradient_x_input}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vc-M9unxfWT9"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "explainer = GradientExplainer(model)\n",
        "vanilla_gradient = explainer.vanilla_gradient(x)\n",
        "gradient_x_input = explainer.gradient_x_input(x)\n",
        "\n",
        "print(f\"Vanilla Gradient: {vanilla_gradient.numpy()}\")\n",
        "print(f\"Gradient x Input: {gradient_x_input.numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySs4w4EYmPa0"
      },
      "source": [
        "**Example 2**\n",
        "\n",
        "In the second example we will analyze the differences in a more sophisticated use case, i.e. classification of handwritten digits. In the following notebook will:\n",
        "\n",
        "1. Load MNIST sample data and perform data preparation steps\n",
        "2. Implement a simple CNN model and training loop\n",
        "3. Train the model for the classification task on the training split and provide validation performance\n",
        "4. Load test instance to explain model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXQlaZmY87aL"
      },
      "outputs": [],
      "source": [
        "from mnist_utils import *\n",
        "\n",
        "# Load data\n",
        "mnist_path = f\"{data_path}/mnist/mnist_train_small.csv\"\n",
        "train_loader, val_loader = load_and_prep_data(mnist_path)\n",
        "\n",
        "# Build model\n",
        "model = LitMNIST_CNN(lr=0.001)\n",
        "\n",
        "# Train model\n",
        "fit_model(model, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfstSJd59gjf"
      },
      "outputs": [],
      "source": [
        "# Load test data to explain\n",
        "test_path = f\"{data_path}/mnist/mnist_test.csv\"\n",
        "test_dat = load_test_data(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1UK6Pp8C17M"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Initialize GradientExplainer object with trained model and generate\n",
        "# vanilla gradient and gradient_x_input explanation of the first instace of the test data\n",
        "\n",
        "explainer = GradientExplainer(model)\n",
        "input, _ = test_dat[0]\n",
        "vanilla_gradient = explainer.vanilla_gradient(input)\n",
        "grad_x_input = explainer.gradient_x_input(input)\n",
        "\n",
        "plot_gradients(input, vanilla_gradient, grad_x_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7R4UidfYcps"
      },
      "source": [
        "# 3. Saliency Maps for DeepDrebin (Gradients & Gradients × Input)\n",
        "\n",
        "This notebook:\n",
        "1. Loads a single **bundle** file (model + selector + selected feature names).\n",
        "2. Loads a filtered **LIBSVM** dataset (labels `-1/+1` → `{0,1}`).\n",
        "3. Splits into **train/val/test** using the same seed/splits stored in the bundle (for comparability).\n",
        "4. Transforms with the bundled **SelectKBest** into the selected-feature space.\n",
        "5. Picks **N malicious** samples (by label) and computes saliency with:\n",
        "   - **Gradients**: $|\\frac{\\partial (\\text{logit})}{\\partial x}|$\n",
        "   - **Gradients × Input**: $|x \\cdot \\frac{\\partial (logit)}{\\partial x}|$\n",
        "6. Prints **top-K feature names** with scores and (optionally) saves a CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwVHva6zWU_o"
      },
      "outputs": [],
      "source": [
        "from drebin_utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISq1XoQjW2Cn"
      },
      "outputs": [],
      "source": [
        "# Set configs\n",
        "DATA_PATH = f\"{data_path}/drebin\"\n",
        "BUNDLE_PATH = os.path.join(DATA_PATH, \"drebin_bundle.pt\")\n",
        "LIBSVM_DATA = os.path.join(DATA_PATH, \"drebin_filtered.libsvm.gz\")\n",
        "NUM_MALWARE = 5\n",
        "TOP_K = 10\n",
        "DEVICE = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_LRcfJ2XT9u"
      },
      "outputs": [],
      "source": [
        "# Build model and load data\n",
        "model, bundle = build_model(BUNDLE_PATH, DEVICE)\n",
        "X, y = load_libsvm(LIBSVM_DATA, bundle[\"selector\"], bundle[\"model_hparams\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZTYIDQ-YbPX"
      },
      "outputs": [],
      "source": [
        "# Pick instances to explain\n",
        "picked_indices, probs_all = pick_malicious_indices(X, y, model, num=NUM_MALWARE)\n",
        "\n",
        "print(f\"Selected {len(picked_indices)} malicious samples:\")\n",
        "for r, i in enumerate(picked_indices, 1):\n",
        "    gt = f\", y={int(y[i])}\" if y is not None else \"\"\n",
        "    print(f\"{r:2d}. idx={i:>6}  prob_mal={probs_all[i]:.4f}{gt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cToD-yN5bG1A"
      },
      "outputs": [],
      "source": [
        "def explain_sample(bundle, i: int, top_k: int = 10) -> dict:\n",
        "    \"\"\"\n",
        "    Explain a single sample using Gradients and Input×Gradients.\n",
        "    Shows only positive (malicious) contributions, sorted descending.\n",
        "    Renders a heatmap for each method with a red→blue colorbar (cells in shades of red).\n",
        "    \"\"\"\n",
        "    x = X[i]\n",
        "\n",
        "    # Your code here\n",
        "    #\n",
        "    explainer = GradientExplainer(model)\n",
        "    grad = explainer.vanilla_gradient(x)\n",
        "    ixg = explainer.gradient_x_input(x)\n",
        "    #grad = compute_gradients(model, x)\n",
        "    #ixg  = compute_input_x_gradients(model, x)\n",
        "\n",
        "    # Keep *only* positive contributions and take top-k\n",
        "    grad_idx = topk_positive(grad, top_k)\n",
        "    ixg_idx  = topk_positive(ixg,  top_k)\n",
        "\n",
        "    # Build lists sorted by descending score (most malicious first)\n",
        "    grad_scores = np.array([grad[j] for j in grad_idx], dtype=float)\n",
        "    grad_names  = [fname(bundle[\"selected_names\"], j) for j in grad_idx]\n",
        "\n",
        "    ixg_scores  = np.array([ixg[j]  for j in ixg_idx], dtype=float)\n",
        "    ixg_names   = [fname(bundle[\"selected_names\"], j) for j in ixg_idx]\n",
        "\n",
        "    # Header\n",
        "    print(f\"\\n=== Sample {i} ===\")\n",
        "    if y is not None:\n",
        "        print(f\"Ground truth label: {int(y[i])} (1=malware)\")\n",
        "    p = float(predict_proba(model, X[i:i+1])[0])\n",
        "    print(f\"Predicted malware probability: {p:.4f}\")\n",
        "\n",
        "    # Heatmaps: most relevant on top (already sorted)\n",
        "    if len(grad_scores):\n",
        "        plot_malicious_heatmap(grad_scores, grad_names, \"Gradients (malicious)\")\n",
        "    else:\n",
        "        print(\"No positive (malicious) gradient contributions found.\")\n",
        "\n",
        "    if len(ixg_scores):\n",
        "        plot_malicious_heatmap(ixg_scores, ixg_names, \"Input × Gradients (malicious)\")\n",
        "    else:\n",
        "        print(\"No positive (malicious) Input×Gradients contributions found.\")\n",
        "\n",
        "    return {\n",
        "        \"sample_index\": i,\n",
        "        \"prob_malware\": p,\n",
        "        \"top_gradients\": [\n",
        "            {\"index\": int(j), \"name\": fname(bundle[\"selected_names\"], j), \"score\": float(grad[j])}\n",
        "            for j in grad_idx\n",
        "        ],\n",
        "        \"top_input_x_gradients\": [\n",
        "            {\"index\": int(j), \"name\": fname(bundle[\"selected_names\"], j), \"score\": float(ixg[j])}\n",
        "            for j in ixg_idx\n",
        "        ],\n",
        "    }\n",
        "\n",
        "# Re-run explanations for the picked malicious samples\n",
        "explanations = [explain_sample(bundle, i, top_k=TOP_K) for i in picked_indices]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
